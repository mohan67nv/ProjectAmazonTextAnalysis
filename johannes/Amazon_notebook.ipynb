{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = 'C:\\\\users\\johannes\\ProjectAmazonTextAnalysis\\johannes'\n",
    "os.chdir(path)\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "import gzip\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000\nStep: 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3000\nStep: 4000\nStep: 5000\nStep: 6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 7000\nStep: 8000\nStep: 9000\nStep: 10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 11000\nStep: 12000\nStep: 13000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 14000\nStep: 15000\nStep: 16000\nStep: 17000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 18000\nStep: 19000\nStep: 20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 2.5546047687530518\n"
     ]
    }
   ],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "sample_size = 1000\n",
    "\n",
    "def get_training_data(path):\n",
    "    \"\"\"\n",
    "    Get all usable data\n",
    "    :param path: path to compressed data\n",
    "    :return: panda data frame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        i += 1\n",
    "        if i <= sample_size:\n",
    "            df[i] = d\n",
    "        else:\n",
    "            break\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"Step:\", i + 1)\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "# def get_test_data(path):\n",
    "#     \"\"\"\n",
    "#     Do not call this before the real test!!!!\n",
    "#     \"\"\"\n",
    "#     pass\n",
    "#     i = 0\n",
    "#     df = {}\n",
    "#     for d in parse(path):\n",
    "#         i += 1\n",
    "#         if i > 1400000:\n",
    "#             df[i] = d\n",
    "#     return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "df = get_training_data('reviews_Electronics_5.json.gz')\n",
    "\n",
    "print(\"Time :\", time.time() - start_time)\n",
    "\n",
    "df_1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_dataframe(df = df_1):\n",
    "    y = df['overall'].values\n",
    "    X = df['reviewText']\n",
    "    df = pd.DataFrame(np.column_stack((X,y)), columns = ['text', 'labels'])\n",
    "    return df\n",
    "df = fix_dataframe(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 2)\n"
     ]
    }
   ],
   "source": [
    "def split_data(df = df):\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    # print(train_df.head())\n",
    "    # return pd.DataFrame(train_df, columns=['text', 'labels']), pd.DataFrame(test_df, columns=['text', 'labels'])\n",
    "    return train_df, test_df\n",
    "train_df, test_df = split_data(df)\n",
    "train_df.head()\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('you', 18018), ('that', 16464), ('in', 16218), ('my', 16008), ('with', 15625), ('on', 13330), ('have', 13251), ('but', 11382), ('not', 10477), (\"n't\", 9861), ('are', 9562), (')', 9295), ('as', 9051), (\"'s\", 8957), ('was', 8848), ('(', 8553), ('be', 7346), ('they', 7187), ('so', 6869), ('if', 6835), ('or', 6610), ('can', 6476), ('!', 6449), ('one', 6378), ('do', 6368), ('at', 6269), ('these', 5930), ('use', 5922), ('very', 5796), ('good', 5521), ('great', 5373), ('your', 5335), ('all', 5284), ('just', 5097), ('lens', 5009), ('from', 4958), ('an', 4808), ('more', 4701), ('when', 4641), ('get', 4549), ('had', 4546), ('like', 4487), ('would', 4486), ('will', 4438), ('up', 4354), ('them', 4343), ('out', 4323), (';', 4262), ('camera', 4141), ('has', 4129), ('no', 4012), ('than', 3970), ('well', 3834), ('about', 3815), ('me', 3739), ('&', 3680), ('quality', 3510), ('does', 3481), ('only', 3470), ('...', 3430), ('price', 3351), ('which', 3257), ('what', 3256), ('sound', 3217), ('other', 3214), ('some', 3214), ('there', 3158), ('also', 2945), ('cable', 2914), ('much', 2912), ('works', 2910), (':', 2887), (\"'ve\", 2828), ('headphones', 2807), ('bought', 2788), ('-', 2727), ('time', 2690), ('work', 2655), ('really', 2614), ('any', 2595), ('used', 2564), ('better', 2557), ('even', 2518), ('did', 2485), ('product', 2418), ('need', 2412), ('using', 2375), ('because', 2354), ('buy', 2297), (\"'m\", 2295), ('by', 2287), ('am', 2233), ('now', 2228), ('after', 2189), ('little', 2185), ('could', 2144), (\"''\", 2141), ('$', 2116), ('still', 2109), ('want', 2031)]\n"
     ]
    }
   ],
   "source": [
    "# Using the standard stopwords given by nltk. Can also do feature relevance according to word frequency limits.\n",
    "# Could also test a limit for word appearance in a given percentage of texts.\n",
    "\n",
    "def find_words(df = train_df, stopword = False, word_frequency = [sample_size, np.log(sample_size)]):\n",
    "    # stemmer = SnowballStemmer('english')\n",
    "    \n",
    "    texts = df['text'].values\n",
    "    # dictionary = np.unique([word.lower() for text in texts for word in word_tokenize(text)])\n",
    "    # word_count = Counter([word.lower() for text in texts for word in word_tokenize(text)])\n",
    "    \n",
    "    if stopword == False:\n",
    "        word_count = Counter([word.lower() for text in texts \n",
    "                              for word in word_tokenize(text)])\n",
    "        if word_frequency != None:\n",
    "            word_count = {word: count for word, count in word_count.items() \n",
    "                          if count < word_frequency[0] and count > word_frequency[1]}            \n",
    "    elif stopword == True:\n",
    "        word_count = Counter([word.lower() \n",
    "                                for text in texts \n",
    "                                for word in word_tokenize(text) \n",
    "                                if word not in stopwords.words('english')])\n",
    "    else:\n",
    "        raise ValueError('stopword argument needs to be True/False')\n",
    "    \n",
    "    dictionary = [word for word, count in word_count.items()]\n",
    "    word_count = sorted([(word, count) for word, count in word_count.items()], key = lambda x: -x[1])\n",
    "    return word_count, dictionary\n",
    "word_freq, dictionary = find_words()\n",
    "print(word_freq[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(words):\n",
    "    return zip(words, words[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('.', 'i'), 12988), ((',', 'and'), 7786), (('.', 'the'), 7164), ((',', 'but'), 6168), (('of', 'the'), 5786), ((',', 'i'), 5192), (('.', 'it'), 5167), (('i', 'have'), 4630), (('it', \"'s\"), 4262), (('on', 'the'), 4031), (('if', 'you'), 3822), (('in', 'the'), 3817), (('it', 'is'), 3416), (('is', 'a'), 3344), (('for', 'the'), 3252), (('to', 'the'), 3153), (('and', 'the'), 3130), ((',', 'the'), 3125), (('with', 'the'), 3120), (('do', \"n't\"), 3030), (('this', 'is'), 2929), ((',', 'it'), 2744), (('and', 'i'), 2651), (('i', \"'ve\"), 2399), (('.', 'this'), 2359), (('to', 'be'), 2307), (('for', 'a'), 2274), (('you', 'can'), 2247), (('i', \"'m\"), 2172), (('and', 'it'), 2167), (('it', '.'), 2161), (('i', 'was'), 2149), (('with', 'a'), 2010), ((')', '.'), 1968), (('i', 'bought'), 1797), (('.', 'if'), 1756), (('to', 'use'), 1741), (('i', 'am'), 1738), (('that', 'i'), 1737), (('but', 'i'), 1698), ((',', 'so'), 1690), (('have', 'a'), 1644), (('.', 'they'), 1638), (('have', 'to'), 1637), ((',', 'you'), 1620), (('they', 'are'), 1608), (('i', 'do'), 1601), (('i', 'would'), 1595), (('it', 'was'), 1583), (('i', 'had'), 1577), (('the', 'price'), 1567), (('to', 'get'), 1558), (('&', 'quot'), 1541), (('quot', ';'), 1541), (('i', 'can'), 1538), (('is', 'the'), 1493), (('so', 'i'), 1491), (('for', 'my'), 1449), (('when', 'i'), 1410), (('in', 'a'), 1393), (('the', 'camera'), 1376), (('a', 'lot'), 1332), (('a', 'good'), 1325), (('on', 'my'), 1324), (('.', 'you'), 1314), (('ca', \"n't\"), 1311), (('in', 'my'), 1303), (('but', 'it'), 1293), (('easy', 'to'), 1281), (('that', 'the'), 1274), (('a', 'great'), 1271), ((',', 'which'), 1268), (('a', 'little'), 1266), (('.', 'but'), 1258), ((',', 'this'), 1258), (('the', 'same'), 1247), (('it', 'does'), 1228), ((')', ','), 1204), (('a', 'few'), 1196), (('!', '!'), 1195), (('from', 'the'), 1183), (('you', 'have'), 1180), (('want', 'to'), 1173), (('at', 'the'), 1166), (('does', \"n't\"), 1163), (('i', 'use'), 1130), (('this', 'lens'), 1125), (('is', 'not'), 1123), (('on', 'a'), 1097), (('it', 'to'), 1095), (('you', 'are'), 1094), (('one', 'of'), 1072), (('the', 'best'), 1070), (('did', \"n't\"), 1039), (('all', 'the'), 1022), (('of', 'my'), 1016), (('as', 'a'), 1012), (('out', 'of'), 1012), (('with', 'this'), 1005), (('with', 'my'), 999)]\n"
     ]
    }
   ],
   "source": [
    "def get_bigrams(df = train_df ,lower_limit = np.log(sample_size)):\n",
    "    texts = df['text'].values\n",
    "    # lower case text\n",
    "    texts_lower = [[word.lower() for word in word_tokenize(text)] for text in texts]\n",
    "    # bigrams from the lower case text\n",
    "    bigrams = [gram for text in texts_lower for gram in find_bigrams(text)]\n",
    "    # Count of bigrams sorted\n",
    "    bigram_count = Counter(bigrams)\n",
    "    bigrams = [bigram for bigram, count in bigram_count.items() if count > lower_limit]\n",
    "    sorted_bigrams = sorted([(bigram, count) \n",
    "                             for bigram, count in bigram_count.items() \n",
    "                             if bigram in bigrams], \n",
    "                            key = lambda x: -x[1])\n",
    "    \n",
    "    return bigrams, sorted_bigrams, texts_lower\n",
    "    # (',', 'but) , ('do', \"n't\"), ('the', 'price'), ('.', 'if'), ('but', 'it'), ('did', \"n't\"), \n",
    "bigrams, bigram_count, texts_lower = get_bigrams()\n",
    "# print(bigrams)\n",
    "print(bigram_count[:100]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_dataframe(texts = train_df.text.values, \n",
    "                   words = dictionary):\n",
    "    word_occurances = []\n",
    "    for text in texts:\n",
    "        text_occurences = np.zeros(len(words))\n",
    "        for word in word_tokenize(text):\n",
    "            word = word.lower()\n",
    "            if word in words:\n",
    "                index = words.index(word)\n",
    "                text_occurences[index] += 1\n",
    "        word_occurances.append(text_occurences)\n",
    "    \n",
    "    X_words = pd.DataFrame(np.array(word_occurances), columns = words)\n",
    "    return X_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_dataframe(texts = train_df.text.values, \n",
    "                   words = dictionary):\n",
    "    word_occurances = []\n",
    "    texts_lower = [[word.lower() for word in word_tokenize(text)] for text in texts]\n",
    "    \n",
    "    for text in texts_lower:\n",
    "        text_occurences = np.zeros(len(words))\n",
    "        for word in text:\n",
    "            if word in words:\n",
    "                index = words.index(word)\n",
    "                text_occurences[index] += 1\n",
    "        word_occurances.append(text_occurences)\n",
    "    \n",
    "    X_words = pd.DataFrame(np.array(word_occurances), columns = words)\n",
    "    return X_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('as', 'well')</th>\n",
       "      <th>('a', 'great')</th>\n",
       "      <th>(',', 'i')</th>\n",
       "      <th>('i', 'was')</th>\n",
       "      <th>('to', 'mount')</th>\n",
       "      <th>('.', 'i')</th>\n",
       "      <th>(';', 'tv')</th>\n",
       "      <th>('.', 'also')</th>\n",
       "      <th>('like', 'the')</th>\n",
       "      <th>('have', 'a')</th>\n",
       "      <th>...</th>\n",
       "      <th>('and', 'i')</th>\n",
       "      <th>('for', 'the')</th>\n",
       "      <th>('tv', 'mount')</th>\n",
       "      <th>('move', 'the')</th>\n",
       "      <th>('garmin', '.')</th>\n",
       "      <th>('of', 'this')</th>\n",
       "      <th>('have', 'to')</th>\n",
       "      <th>('truck', 'specific')</th>\n",
       "      <th>('tv', '.')</th>\n",
       "      <th>('.', 'the')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>('as', 'well')</th>\n",
       "      <th>('a', 'great')</th>\n",
       "      <th>(',', 'i')</th>\n",
       "      <th>('i', 'was')</th>\n",
       "      <th>('to', 'mount')</th>\n",
       "      <th>('.', 'i')</th>\n",
       "      <th>(';', 'tv')</th>\n",
       "      <th>('.', 'also')</th>\n",
       "      <th>('like', 'the')</th>\n",
       "      <th>('have', 'a')</th>\n",
       "      <th>...</th>\n",
       "      <th>('and', 'i')</th>\n",
       "      <th>('for', 'the')</th>\n",
       "      <th>('tv', 'mount')</th>\n",
       "      <th>('move', 'the')</th>\n",
       "      <th>('garmin', '.')</th>\n",
       "      <th>('of', 'this')</th>\n",
       "      <th>('have', 'to')</th>\n",
       "      <th>('truck', 'specific')</th>\n",
       "      <th>('tv', '.')</th>\n",
       "      <th>('.', 'the')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 147 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigram_dataframe(texts = train_df.text.values, \n",
    "                   bigrams = bigrams):\n",
    "    bigram_occurances = []\n",
    "    for text in texts:\n",
    "        text_occurences = np.zeros(len(bigrams))\n",
    "        text_words = [word.lower() for word in word_tokenize(text)]\n",
    "        bigrams_in_text = [gram for gram in find_bigrams(text_words)]\n",
    "        for gram in bigrams_in_text:\n",
    "            if gram in bigrams:\n",
    "                index = bigrams.index(gram)\n",
    "                text_occurences[index] += 1\n",
    "        bigram_occurances.append(text_occurences)\n",
    "    \n",
    "    cols = [str(gram) for gram in bigrams]\n",
    "    # print(cols)\n",
    "    X_bigrams = pd.DataFrame(np.array(bigram_occurances), columns = cols)\n",
    "    return X_bigrams\n",
    "_ = bigram_dataframe()\n",
    "_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5019c666a52c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;31m# Dataframes of bigrams/unigrams in the train and test datasets.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_bigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigram_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_bigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigram_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_unigrams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-daa8f9c63a9c>\u001b[0m in \u001b[0;36mbigram_dataframe\u001b[0;34m(texts, bigrams)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mgram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams_in_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgram\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigrams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgram\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mtext_occurences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbigram_occurances\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_occurences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Dataframes of bigrams/unigrams in the train and test datasets.\n",
    "train_bigrams = bigram_dataframe(texts = train_df.text.values)\n",
    "test_bigrams = bigram_dataframe(texts = test_df.text.values)\n",
    "\n",
    "train_unigrams = word_dataframe(texts = train_df.text.values)\n",
    "test_unigrams = word_dataframe(texts = test_df.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   screws  mount  was  very  what   's   to   be  some   as ...   needed   in  \\\n0     0.0    0.0  0.0   1.0   0.0  1.0  0.0  0.0   0.0  0.0 ...      0.0  0.0   \n1     0.0    5.0  0.0   0.0   0.0  0.0  3.0  0.0   0.0  0.0 ...      0.0  0.0   \n2     0.0    1.0  0.0   0.0   0.0  0.0  3.0  0.0   0.0  0.0 ...      1.0  2.0   \n3     0.0    0.0  0.0   0.0   0.0  0.0  2.0  0.0   0.0  0.0 ...      0.0  0.0   \n4     0.0    0.0  4.0   0.0   0.0  0.0  3.0  2.0   0.0  0.0 ...      1.0  0.0   \n\n   easy   do  can  them  and  bought  price  n't  \n0   0.0  0.0  0.0   0.0  0.0     0.0    0.0  0.0  \n1   1.0  0.0  0.0   1.0  2.0     0.0    0.0  0.0  \n2   1.0  0.0  0.0   0.0  0.0     0.0    0.0  0.0  \n3   1.0  0.0  0.0   0.0  2.0     0.0    0.0  0.0  \n4   0.0  0.0  0.0   0.0  0.0     0.0    0.0  0.0  \n\n[5 rows x 95 columns]\n   screws  mount   was  very  what   's    to   be  some   as ...   needed  \\\n0     0.0    0.0  10.0   2.0   7.0  5.0  24.0  6.0   1.0  6.0 ...      0.0   \n1     2.0    1.0   0.0   0.0   0.0  0.0   3.0  2.0   0.0  0.0 ...      0.0   \n2     0.0    0.0   2.0   0.0   0.0  1.0   0.0  0.0   1.0  0.0 ...      0.0   \n3     0.0    0.0   9.0   0.0   1.0  0.0  15.0  1.0   2.0  1.0 ...      1.0   \n4     0.0    0.0   0.0   0.0   0.0  1.0   0.0  0.0   0.0  1.0 ...      0.0   \n\n    in  easy   do  can  them   and  bought  price  n't  \n0  7.0   0.0  5.0  6.0   0.0  19.0     0.0    2.0  1.0  \n1  0.0   0.0  1.0  0.0   0.0   2.0     0.0    0.0  2.0  \n2  0.0   0.0  0.0  0.0   0.0   2.0     2.0    0.0  0.0  \n3  8.0   0.0  0.0  0.0   1.0  13.0     1.0    0.0  3.0  \n4  2.0   0.0  0.0  0.0   0.0   1.0     1.0    0.0  0.0  \n\n[5 rows x 95 columns]\n####################\n(75, 95)\n(25, 95)\n"
     ]
    }
   ],
   "source": [
    "# print(train_unigrams.head())\n",
    "# print(test_unigrams.head())\n",
    "# print(20*'#')\n",
    "# print(train_unigrams.shape)\n",
    "# print(test_unigrams.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the bigrams and unigrams dataframes to one dataframe. \n",
    "def merge_grams(unigrams, bigrams):\n",
    "    combined = pd.concat([bigrams, unigrams], axis = 1)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = merge_grams(train_bigrams, train_unigrams)\n",
    "X_test = merge_grams(test_bigrams, test_unigrams)\n",
    "y_train = train_df['labels']\n",
    "y_test = test_df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')\n",
    "y_train.to_pickle('y_train.pkl')\n",
    "y_test.to_pickle('y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}