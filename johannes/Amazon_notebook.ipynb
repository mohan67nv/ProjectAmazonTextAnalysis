{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "path = 'C:\\\\users\\johannes\\ProjectAmazonTextAnalysis\\johannes'\n",
    "os.chdir(path)\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter\n",
    "import gzip\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000\nStep: 2000\nStep: 3000\nStep: 4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000\nStep: 6000\nStep: 7000\nStep: 8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 9000\nStep: 10000\nStep: 11000\nStep: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 13000\nStep: 14000\nStep: 15000\nStep: 16000\nStep: 17000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 18000\nStep: 19000\nStep: 20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 1.4663589000701904\n"
     ]
    }
   ],
   "source": [
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "sample_size = 20000\n",
    "\n",
    "def get_training_data(path):\n",
    "    \"\"\"\n",
    "    Get all usable data\n",
    "    :param path: path to compressed data\n",
    "    :return: panda data frame\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        i += 1\n",
    "        if i <= sample_size:\n",
    "            df[i] = d\n",
    "        else:\n",
    "            break\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"Step:\", i + 1)\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "# def get_test_data(path):\n",
    "#     \"\"\"\n",
    "#     Do not call this before the real test!!!!\n",
    "#     \"\"\"\n",
    "#     pass\n",
    "#     i = 0\n",
    "#     df = {}\n",
    "#     for d in parse(path):\n",
    "#         i += 1\n",
    "#         if i > 1400000:\n",
    "#             df[i] = d\n",
    "#     return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "df = get_training_data('reviews_Electronics_5.json.gz')\n",
    "\n",
    "print(\"Time :\", time.time() - start_time)\n",
    "\n",
    "df_1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def fix_dataframe(df = df_1):\n",
    "    y = df['overall'].values\n",
    "    X = df['reviewText']\n",
    "    df = pd.DataFrame(np.column_stack((X,y)), columns = ['text', 'review_labels'])\n",
    "    return df\n",
    "df = fix_dataframe(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n(15000, 2)\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def split_data(df = df):\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    # print(train_df.head())\n",
    "    # return pd.DataFrame(train_df, columns=['text', 'labels']), pd.DataFrame(test_df, columns=['text', 'labels'])\n",
    "    return train_df, test_df\n",
    "train_df, test_df = split_data(df)\n",
    "train_df.head()\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first_time: 12.586816787719727\nTotal time: 12.59432053565979\n"
     ]
    }
   ],
   "source": [
    "# Using the standard stopwords given by nltk. Can also do feature relevance according to word frequency limits.\n",
    "# Could also test a limit for word appearance in a given percentage of texts.\n",
    "start_time = time.time()\n",
    "def find_words(df = train_df, \n",
    "               stopword = False, \n",
    "               word_frequency = [sample_size, np.log(sample_size)], \n",
    "               number_of_words = 3000):\n",
    "    # stemmer = SnowballStemmer('english')\n",
    "    start_time = time.time()\n",
    "    texts = df['text'].values\n",
    "    # dictionary = np.unique([word.lower() for text in texts for word in word_tokenize(text)])\n",
    "    # word_count = Counter([word.lower() for text in texts for word in word_tokenize(text)])\n",
    "    \n",
    "    if stopword == False:\n",
    "        word_count = Counter([word.lower() for text in texts \n",
    "                              for word in word_tokenize(text)])\n",
    "        if word_frequency != None:\n",
    "            word_count = {word: count for word, count in word_count.items() \n",
    "                          if count < word_frequency[0] and count > word_frequency[1]}            \n",
    "    elif stopword == True:\n",
    "        word_count = Counter([word.lower() \n",
    "                                for text in texts \n",
    "                                for word in word_tokenize(text) \n",
    "                                if word not in stopwords.words('english')])\n",
    "    else:\n",
    "        raise ValueError('stopword argument needs to be True/False')\n",
    "    print('first_time:', time.time() - start_time)\n",
    "    dictionary = [word for word, count in word_count.items()]\n",
    "    word_count = sorted([(word, count) for word, count in word_count.items()], key = lambda x: -x[1])\n",
    "    return word_count, dictionary[:number_of_words]\n",
    "word_freq, dictionary = find_words()\n",
    "# print(word_freq[:100])\n",
    "print('Total time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bigrams(words):\n",
    "    return zip(words, words[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 13.651100873947144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 13.588640213012695\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "def get_bigrams(df=train_df, \n",
    "                lower_limit=np.log(sample_size), \n",
    "                upper_limit = sample_size, \n",
    "                number_of_bigrams = 1000):\n",
    "    \n",
    "    texts = df['text'].values\n",
    "    # lower case text\n",
    "    texts_lower = [[word.lower() for word in word_tokenize(text)] for text in texts]\n",
    "    # bigrams from the lower case text\n",
    "    bigrams = [gram for text in texts_lower for gram in find_bigrams(text)]\n",
    "    # Count of bigrams sorted\n",
    "    bigram_count = Counter(bigrams)\n",
    "    \n",
    "    # start_time = time.time()\n",
    "    # bigrams = [bigram for bigram, count in bigram_count.items() if count > lower_limit]\n",
    "    \n",
    "    sorted_bigrams = sorted([(bigram, count)\n",
    "                             for bigram, count in bigram_count.items()\n",
    "                             if count > lower_limit and count < upper_limit],\n",
    "                            key=lambda x: -x[1])\n",
    "    bigrams = [bigram for bigram, count in sorted_bigrams]\n",
    "    ######### sorted_bigrams = np.sort(np.array(bigram_count.items())\n",
    "    # sorted_bigrams = sorted(bigram_count.items(), key = lambda x: -x[1])\n",
    "    # sorted_bigrams = [i for i in sorted_bigrams if i[0] in bigrams]\n",
    "    # print('sort bigram time:', time.time() - start_time)\n",
    "\n",
    "    return bigrams[:number_of_bigrams], sorted_bigrams, texts_lower\n",
    "    # (',', 'but) , ('do', \"n't\"), ('the', 'price'), ('.', 'if'), ('but', 'it'), ('did', \"n't\"), \n",
    "bigrams, bigram_count, texts_lower = get_bigrams()\n",
    "# print(bigrams)\n",
    "# print(bigram_count[:100])\n",
    "print('Total time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# \n",
    "# \n",
    "# def word_dataframe(texts = train_df.text.values, \n",
    "#                    words = dictionary):\n",
    "#     word_occurances = []\n",
    "#     start_time = time.time()\n",
    "#     # texts_lower = [[word.lower() for word in text] for text in texts]\n",
    "#     for text in texts:\n",
    "#         text_occurences = np.zeros(len(words))\n",
    "#         for word in word_tokenize(text):\n",
    "#             word = word.lower()\n",
    "#             if word in words:\n",
    "#                 index = words.index(word)\n",
    "#                 text_occurences[index] += 1\n",
    "#         word_occurances.append(text_occurences)\n",
    "#     print('loop time:', time.time() - start_time)\n",
    "#     X_words = pd.DataFrame(np.array(word_occurances), columns = words)\n",
    "#     return X_words\n",
    "# \n",
    "# \n",
    "# _ = word_dataframe()\n",
    "# \n",
    "# print('Total time:', time.time() - start_time)\n",
    "# _.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "def word_dataframe(texts = train_df.text.values, \n",
    "                   words = dictionary):\n",
    "    texts_lower = [[word.lower() for word in word_tokenize(text)] for text in texts]\n",
    "    counts = [Counter(text) for text in texts_lower]\n",
    "    word_occurances = np.array([[counts[i][word] for word in words] for i in range(len(counts))])\n",
    "    X_words = pd.DataFrame(word_occurances, columns = words)\n",
    "    return X_words\n",
    "# _ = word_dataframe()\n",
    "# print('total time:', time.time() - start_time)\n",
    "# _.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# \n",
    "# def bigram_dataframe(texts = train_df.text.values, \n",
    "#                    bigrams = bigrams):\n",
    "#     bigram_occurances = []\n",
    "#     for text in texts:\n",
    "#         text_occurences = np.zeros(len(bigrams))\n",
    "#         text_words = [word.lower() for word in word_tokenize(text)]\n",
    "#         bigrams_in_text = [gram for gram in find_bigrams(text_words)]\n",
    "#         for gram in bigrams_in_text:\n",
    "#             if gram in bigrams:\n",
    "#                 index = bigrams.index(gram)\n",
    "#                 text_occurences[index] += 1\n",
    "#         bigram_occurances.append(text_occurences)\n",
    "#     \n",
    "#     cols = [str(gram) for gram in bigrams]\n",
    "#     # print(cols)\n",
    "#     X_bigrams = pd.DataFrame(np.array(bigram_occurances), columns = cols)\n",
    "#     return X_bigrams\n",
    "# _ = bigram_dataframe()\n",
    "# print('Total time:', time.time() - start_time)\n",
    "# _.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "def bigram_dataframe(texts = train_df.text.values, \n",
    "                   bigrams = bigrams):\n",
    "    texts_lower = [[word.lower() for word in word_tokenize(text)] for text in texts]\n",
    "    # print(texts_lower[0])\n",
    "    bigrams_in_text = [[gram for gram in find_bigrams(text)] for text in texts_lower]\n",
    "    bigram_counts = [Counter(gram) for gram in bigrams_in_text]\n",
    "    bigram_occurances = np.array([[bigram_counts[i][gram] for gram in bigrams] for i in range(len(bigram_counts))])\n",
    "\n",
    "    cols = [str(gram) for gram in bigrams]\n",
    "    X_bigrams = pd.DataFrame(np.array(bigram_occurances), columns = cols)\n",
    "    return X_bigrams\n",
    "# _ = bigram_dataframe()\n",
    "# print('total time:', time.time() - start_time)\n",
    "# _.head()\n",
    "# bigram_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 78.97760486602783\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Dataframes of bigrams/unigrams in the train and test datasets.\n",
    "train_bigrams = bigram_dataframe(texts = train_df.text.values)\n",
    "test_bigrams = bigram_dataframe(texts = test_df.text.values)\n",
    "\n",
    "train_unigrams = word_dataframe(texts = train_df.text.values)\n",
    "test_unigrams = word_dataframe(texts = test_df.text.values)\n",
    "\n",
    "print('Total time:', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   considerably  communicate  fixed  ideal  remembered  invested  distort  \\\n0             0            0      0      0           0         0        0   \n1             0            0      0      0           0         0        0   \n2             0            0      0      0           0         0        0   \n3             0            0      0      0           0         0        0   \n4             0            0      0      0           0         0        0   \n\n   perspective  rom  v2    ...     conversion  tripplite  was  regarding  \\\n0            0    0   0    ...              0          0    0          0   \n1            0    0   0    ...              0          0    0          0   \n2            0    0   0    ...              0          0    0          0   \n3            0    0   0    ...              0          0    0          0   \n4            0    0   0    ...              0          0    0          0   \n\n   proprietary  lens.i  recharges  pal  reception  lifespan  \n0            0       0          0    0          0         0  \n1            0       0          0    0          0         0  \n2            0       0          0    0          0         0  \n3            0       0          0    0          0         0  \n4            0       0          0    0          0         0  \n\n[5 rows x 3000 columns]\n   ('.', 'i')  (',', 'and')  ('.', 'the')  (',', 'but')  ('of', 'the')  \\\n0           0             1             0             0              0   \n1           0             1             0             0              0   \n2           0             1             0             1              0   \n3           0             0             0             0              0   \n4           0             0             0             0              0   \n\n   ('.', 'it')  (',', 'i')  ('i', 'have')  ('it', \"'s\")  ('on', 'the')  \\\n0            0           0              0             0              0   \n1            0           0              0             0              0   \n2            0           0              0             0              0   \n3            0           0              0             0              0   \n4            0           0              0             0              0   \n\n       ...        ('my', 'head')  ('a', 'more')  ('for', 'over')  ('to', '.')  \\\n0      ...                     0              0                0            0   \n1      ...                     0              0                0            0   \n2      ...                     0              0                0            0   \n3      ...                     0              0                0            0   \n4      ...                     0              0                0            0   \n\n   ('light', ',')  ('and', 'all')  ('love', 'this')  ('a', 'must')  \\\n0               0               0                 0              0   \n1               0               0                 0              0   \n2               0               0                 0              0   \n3               0               0                 0              0   \n4               0               0                 0              0   \n\n   ('my', 'lens')  ('got', 'it')  \n0               0              0  \n1               0              0  \n2               0              0  \n3               0              0  \n4               0              0  \n\n[5 rows x 1000 columns]\n####################\n(15000, 3000)\n(5000, 3000)\n"
     ]
    }
   ],
   "source": [
    "print(train_unigrams.head())\n",
    "print(train_bigrams.head())\n",
    "print(20*'#')\n",
    "print(train_unigrams.shape)\n",
    "print(test_unigrams.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the bigrams and unigrams dataframes to one dataframe. \n",
    "def merge_grams(unigrams, bigrams):\n",
    "    combined = pd.concat([bigrams, unigrams], axis = 1)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000,)\n(5000,)\n"
     ]
    }
   ],
   "source": [
    "# Get the length of each review\n",
    "train_lengths = [len([word for word in word_tokenize(text)]) for text in train_df.text.values]\n",
    "test_lengths = [len([word for word in word_tokenize(text)]) for text in test_df.text.values]\n",
    "print(np.array(train_lengths).shape)\n",
    "print(np.array(test_lengths).shape)\n",
    "# lengths_test = [len(text) for text in word_tokenize(test_df.text.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = merge_grams(train_bigrams, train_unigrams)\n",
    "X_test = merge_grams(test_bigrams, test_unigrams)\n",
    "y_train = train_df['review_labels']\n",
    "y_test = test_df['review_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['review_lenght'] = train_lengths\n",
    "X_test['review_lenght'] = test_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')\n",
    "y_train.to_pickle('y_train.pkl')\n",
    "y_test.to_pickle('y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}